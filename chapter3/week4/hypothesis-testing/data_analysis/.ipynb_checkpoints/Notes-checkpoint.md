## DATA ANALYSIS: 

When you already have data, and want to make decision based upon those data, then such process is called data anaylysis. This is not about making decision under uncertainity. If you had to make decision under uncertainity, then we called it as statistical inference. 

# OVERVIEW OF ANALYSIS
Take two examples 
1. Causal research question: Can fast food impact health of teenagers ? This investigates about cause and effect relationship. 
2. Correlation research question: Is there any relationship between income of parents and visa acceptance for further study ? This investigates correlation between two variables.

# STEPS TO FOLLOW 
1. <b>Write your hypothesis and plan your research design</b>:  To investigate a relationship between variables within population, we begin with prediction and use statistical method to test that prediction. We will set null and alternative hypothesis. These hypothesis are tested using sample data. These hypothesis are based on prediction related to population  but to test/verify it we use sample data. For "Can fast food impact health of teenagers" , H0: "There is no any impact in health of teenager by consuming fast food once a week", H1: "There is significant impact in health of teenager by consuming fast food once a week." Similary for "Is there any relationship between income of parents and visa acceptance for further study" H0: "There is no any relationship between income and visa acceptance" , H1: "There exists a positive coorelation(when income high, visa accept high) between income and visa acceptance." RESEARCH DESIGN can be done by making sure whether research will use descriptive,correlational and experimental design. Experimental directly influence variable you want to study and Descriptive and correlational studies only measure variables. In  1. Experimental Design:  assess cause-effect-relationship by using statistical test of comparison or regression.
 2. Correlational Design: explores relationship between variables where no any assumption is made.
 3. Descriptive Design: study characteristics of population using statistical tests to draw inference from sample data.
Here, we will also compare the participants at the group or individual or both. It includes, between-subjects , within-subjects, mixed

    1. Within-subject(repeated measure): Here all the participants are introduced to all variables you are trying to measure. For example: if the study is done based on relationship between exercise and memory. In your research exercise has 2 independent variables i.e jogging and yoga. In case of within subject all participants take part in jogging and take memory test as well as same participant take part in yoga and take memory test. We do not divide groups. Same participants take part in both measures. Our test will be which exercise will have siginificant impact on memory.  2*2 within subject design means factorial design where we can look at the effects of 2 or more independent variables. In 2*2 design, two types of therapy and two levels of each treatment (short-term or long-term). Good: when we have limited participants, minimize noise in your study. (Here, the same individual is presented with all the conditions so if his mood is bad in one condition, overall conditions it will be bad but in case oof between subject good mood exposed to only one condition so greater result due to mood in this condition but in another condition they are not exposed so couldn't study their impact. Bad: Carryover effect, i.e when one do yoga and take test then he/she does jogging and take a test. The second test might come good because they also have done yoga already so the effect of first variable can impact in later test. GIST:ALL PARTICIPANTS IN SURVEY RECEIVES EVERY TREATMENT OR VARIABLES WE ARE TRYING TO MEASURE.
    2. Between Subject(between-groups): Here participants are assigned to single treatment only. One group takes part in jogging and another group takes part in yoga. Then test can be done to determine which exercise will have influences on memory.
    3. Mixed (factorial) design: One variable is altered between subjects and another is aletered within subject design.

In case of cause and effect relationship example: You can design within subject design experiment where same group of individual health is measures in 1 week and then the same individual take test after having fast food in a week. One test scores of health before consuming and another after consuming. Independent variable is consumption of fast food. Dependent variable is health test before and after consumption.

In case of correlational, we don't have dependent and independent variable. Rather we  just want to measure  variables without influencing them in any way. 

So now comes topic of measuring variables. There are quantitative and categorical variable which will be described in detail below. In case of cause and effect study: age (ratio), test scores(interval) are quantitative, gender and ethnicity are quantitative (nominal) variables. In case of correlational study:  parental income (ratio) and visa acceptance (binary) quantitative variables. 

2. Collection of data from sample: Sampling can be done by probability and non-probability sampling.  You should have sufficient sample size. Too small cannot be representative and too large expensive than what was necessary. Significance level(the risk you are willing to take to reject null hypothesis when it is true usually 5%) , statistical power(the probability of your study detecting an effect of a certain size if there is one, usually 80% or higher), expected effect size, population std deviation. 

3. Summarize data using descriptive statistics: Using (mean, std deviation,variance, range) , displaying in bar chart/scatter plot.

4. Test hypothesis or make estimates using inferential statistics: Hypothesis testing, parametric testing, point estimate and interval estimate.

5. Interpret your result: Determine statistical significance(p-value < significance level), effect size (though result is significant you need to determine its significance in real life applications, practical significance of your results.  In case of causal-effect relationship Cohen's d is used whereas in correlational study pearson's correlation coefficient is used.

# NOTES
Population: whole number of items like whole population, entire group you want your conclusion about
Sample: selected number of items from population, specific group of people you will work with
Statistic: measure taken from sample, so it describes sample
Parameter: measure concerned a population, so it describes population
Sampling: There are probability and non-probability sampling. Non-probability samples are choosen for specific criteria. Non-probability used in qualitative and exploratory research. often used method but proability sampling such as simple probability sampling and stratified can be used to reduce the risk of sampling bias. Probability sampling used in quantitative research.
Sampling error :  difference between population parameter and sample statistic.
Test statistic: Any value or number obtained from statistical test. It describes how far your observation is from the mean.  It tells how different two or more groups are from overall population mean. The test statistic is used to assess the evidence against the null hypothesis. It quantifies how much the observed data deviates from what would be expected under the assumption of no relationship or no difference. The test statistic provides a measure of how "far" the observed data is from what would be expected if the null hypothesis were true.
Significance level: It is predetermined value choosen before begining our expt. It actually tells us how much we are willing to reject null hypothesis even if it is true. Usually set to small value. If 0.05 = 5% which means we are willing to accept 5% chance of reject true null hypothesis.In about 5 out of 100 experiments, the evidence from the sample might be misleading, leading to the incorrect rejection of a true null hypothesis. So basically, significance level is some threshold researcher set while conducting experiment.  if it is set to 0.05, it means that they are accepting 5% error in accepting the result rejecting the true null hypothesis.If we repeated same experiment out of 100 times only 5 times we will see rejecting H0 even if it is true .

p-value: it is the strength of evidence against null hypothesis. if this is small then significance level, that means the observed data is very less likely to occur under true null hypothesis so we can reject the null hypothesis. smaller p-value means strength is strong enough to reject the null hypothesis. larger p-value means, less strength to reject null hypothesis, that means this observed data are consistent with what would be expected if the null hypothesis were true. 

<b> Small p-value means that the given test statistic is less likely to follow the assumption of null hypothesis being true. small p-value means that we wont be seeing that data under the distribution when we assume that H0 is true. A small p-value indicates that the observed data are less likely to have occurred under the assumption that the null hypothesis is true. In other words, it suggests that the null hypothesis is not a good explanation for the observed results. </b>

So, for data analayis we need to know about: 
1. WHAT KINDS OF DATA i.e DATA TYPE
2. DISTRIBUTIONS OF DATA
3. STATISTICAL TESTS

# DATA TYPES:
Two type of data
1. Qualitative(Categorical): Groupings. non-numerical data having descriptive information like tastes, colors that cannot be counted or measured. Analyzed through interpretations and categorization. Group
2. Quantitative: Amounts. numerical data whose quantity can be measured/counted. Such as height, weight, temperature.Can be expressed in terms of graphs and analyzed through statistical methods.

# QUANTITATIVE 
Different types:
1. Discrete: Distinct values like 1, 2, 3. Do not have any intermediate values in between.  Integer variables. Number of students in the class.
2. Continuous: Ratio variables. infinitely divisible. Between 1 and 2. can take any value between range or interval. Age, volume, Distance.  it is divided into two types:
    1. Interval: No true zero point
    2. Ratio: True zero point
  
# QUALITATIVE
Though they can be represented as numbers, these numbers are simply group representation rather than the count. 
Categorical and Ordinal.
1. Categorical :  Distinct categories or class or groups with no any specific order. Further divided into Binary variables only two possible outcomes or categories or class. (0 or 1 or combination of 0's and 1's) eg of binary: coin flip, yes or no decision. 
2. Ordinal:  Data with natural order among its categories or groups indicating differences or preferences. (1st,2nd or 3rd). Has specific order. EG: finishing place in race, likert scale, rating scale, Star ratings in product is ordinal data but average star rating is quantitative data.
3. Nominal: Groups with no order or rank between them. Eg: Colors, Vehicle type


# INDEPENDENT AND DEPENDENT VARIABLE
We usually conduct experiment to find out  effect of one variable onto another. 
1. Independent variables: Treatment, Variable you think that might be the cause. Exercise is independent variable in memory test. Due to involving in exercise affects memory. The cause of affecting memory is exercise. so exercise = independent variable. You manipulate this variable to affect the outcome of the experiment. 
2. Dependent variables: Response, Variable you think that might be the effect. Memory is dependent variable in memory test. Doing exercise or not doing exercise impacts memory. You are measuring this variable. It is the outcome of the experiment.

In case of correlational research, there is no independent and dependent variables.

# Level of measurement
How precisely variables are recorded.
1. Nominal: Categorized only (Vehicle type,Colors). [=,!=]. [mode (central tendency)] can only be measured.  
2. Ordinal: Categorize and ordered ( Finishing rank in race, likert type questions, very satisfied to dissatisfied, we can order it but can not tell how close or far apart the orders are.). [= & !=  & < or >],[ mode, median], [range(varibility) and interquartile range]
3. Interval: Categorized, ordered, and evenly spaced (test scores), no true zero point. difference between any two adjacent tempertaures in celsius and Fahrenheit is one or greater than 1 but never 0. for eg: 32 and 31 is 1. [ = & !=  & < or >, + ,- ], [mode, median, mean], [range(varibility) and interquartile range, std deviation, variance.] discrete or continuous
4. Ratio: Categorized, ordered, evenly spaced and true zero. Age, weight. True zero = absence of variable of interest. [= & !=  & < or >, + ,- , *,%,], [mode, median, mean, geometric mean] , [range(varibility) and interquartile range, std deviation, variance., relative std deviation], discrete or continuous.
Depending upon measure of variables you can analyze your data.

 Operations | Nominal | Ordinal | Interval | Ratio
| ------------- | ------------- | ------------- | ------------- | ------------- |
| Collection  | By Close ended Questions(gender male or female), open ended Questions (blanks, like age) | close ended question survey | 
|  Descriptive analysis | Frequency distribution table (count or percentage) | Frequency distribution table (count or percentage)  or bar charts  | grouped frequency distribution table or frequency distribution polygon graph | grouped frequency distribution table or frequency distribution polygon graph |
| central tendency | mode | median and mode | mean, median, mode| mean, median, mode, geometric mean |
| variability | - | range | range , std deviation, variance | range , std deviation, variance , relative std. deviation |
|  Statistical Test | chi-square goodness of fit test and chi-square test of independence  | any measurement focused on rankings of measurement they are non-parametric tests | both parametric and non-parametric tests | parametric tests |

## Descriptive Statistics 
Summarize and organize characteristics of a data set. 
1. Distribution: Simple frequency distribution or grouped frequency distribution
2. Central tendency: Center or average of data set. Mean, Median and Mode.
3. Variability: How spread out the response variables are from the center.spread,scatter,dispersion . Range, Std deviation, variance.

NOTE WHILE FINDING STD DEVIATION OF POPULATION DIVIDE BY N BUT WHEN STD DEVIATION OF SAMPLE DIVIDE BY N-1. Inter quartile range Q3-Q1. It helps in determining skewness of distribution. BOX PLOT IS EASIEST TO LOOK AT THE DISTANCE BETWEEN QUARTILES. Negatively skewed median to Q3 less gap. Positively skewed means Q1 and median less gap.


# PROBABILITY DISTRIBUTION
Proability = relative frequency over an infinite number of trials.
A variable can take different values. The probability of different possible values of a variable is called probability distribution. In case of frequency distribution we have number of times of different possible values a variable can take place. 
By law of large numbers, as the sample size increases, the relative frequency of outcomes converges to true probability. that is for more data,the observed frequency in frequency distribution becomes increasingly representative of underlying proability distribution. Frequency distribution is converted by finding the probability of specific value by ratio of frequency of that value to the total number of observations.

1. Probability mass function (discrete variables)
2. Probability density function (continuous variables)


Normally, there are two distributions:
1. Parametric: Assumption is made that data follows a certain specific distribution pattern and parameters of that distribution is estimated from that data.  Normal, Weibull, poisson, exponential  
2. Non-parametric: No any assumption regarding data following certain distribution. Defined without assuming underlying parameters. Uniform, bernoulli, empirical

## Normal distribution 
<ul>
    <li> Bell shaped</li>
    <li>No skewness, centers/symmetric around mean stating that data around mean are more frequent in occurrence than the data far from the mean. most of the values clusters around mean.  </li>
    <li>Mean, median and mode are same</li>
    <li>Symmetric about mean</li>
    <li>Distribution described by mean and std deviation</li>
    
</ul>

Most of all kinds of variables in social sciences are normally distributed. Age, Weight, Test scores, these data are normally distributed.
The CENTRAL LIMIT THEOREM IS THE BASIS ON HOW NORMAL DISTRIBUTION WORK IN STATISTICS.
To get good idea of population, we'd collect multiple random samples within the population. A sampling distribution of the mean is the distribution of the means of these different samples. The central limit theorem states that by using 
1. law of large number, as the sample size increases, the sample mean will approach the population mean.
2. with multiple large samples, sampling distribution of the mean is normally distributed, even if your original variable is not normally distributed. 


Statistical test assumes
1. The data are normally distributed
2. groups that are being compared have similar variance
3. data are independent

For a parametrical statistical tests, it assumes sample comes from normal distribution but according to central limit theorem if sample size is large the requirement of sample to be normally distributed is not neccessary (as the sample size increases sample mean approaches to population mean). So PARAMETRICAL TEST can be used if you have large samples without need to meed of being normally distributed but other 2 assumption should be met. Any sample size > 30 is considered large sample.

Standard normal distribution is called z-distribution. Special normal distribution where mean = 0 and std deviation = 1. Any normal distribution can be converted into standard normal distribution by converting its value into z-scores. z-score tells about a data point is how many std deviation away from mean. If z-score is +ve, a particular data point is above the mean and below if it is -ve.  z-score = 1, means the data point is 1 std deviation above from the mean and z-score = -2 means the data point is 2 std deviation below from the mean.
z = x - populationmean/ std deviation.
WHILE DOING Z-TEST, z-score is the test statistic used in a z-test. z-test is used to compare the means of two groups or to compare the mean of a group to a known value.

## POISSON DISTRIBUTION

Discrete probability distribution, gives probability of a discrete outcome.  Discrete outcome = number of times an event occurs, represented by k. It is particularly useful for situations where events occur independently and at a constant average rate over time or space. Here number of events = disease case, customer purchase.The Poisson distribution is commonly used in various fields, such as queuing theory, telecommunications, biology (for modeling rare events), and reliability engineering. P(X=k)= (e^(-λ)*λ^k)/k! where, P(X=k) Probability of observing k events,λ = average rate of events. k = number of events.

You can use possion distribution when you know that events are happening randomly and independent of each other and the mean number of events occurring within a given interval of time or space.

Specifically for count data. where count data means observations that are non-negative integers. 

Examples: website visits per month, influenza cases per year, text messages per hour.
​

Note: it has only one parameter i.e mean and variance = same = λ

There should be discrete events and given period of time like people arriving in an hour. phone calls in a day. Events are discrete we can count them like how many people, how many phone calls represented by counts. They cannot happen at the same time. λ = rate , number of events per time period.

Example: if your dog brings a toy to you. It can be modeled as poisson distribution.  number of discrete events = toys brought. λ  = 1, 1 toy per week. In order to find, λ  = 4, 4 toy per week.

As the average rate (λ) increases, the shape of the Poisson distribution becomes more skewed and approaches a normal (Gaussian) distribution, thanks to the central limit theorem. The Poisson distribution is often used as an approximation for the binomial distribution when the number of trials is large, and the probability of success is small. The intuition behind the Poisson distribution comes from situations where rare events happen independently over time or space.

It is discrete, normal is continuous. The shape of poisson distribution depends upon λ. Normal is symmetrical around mean and poisson distribution is right skewed.λ increase, asymmetry decrease. 

## CHI SQUARE DISTRIBUTION

It is continuous probability distribution so has probability density function. It is widely used in hypothesis testing chi squared goodness of fit and chi squared test of independence. The shape of chi-square distribution is determined by k, here k = degree of freedom. Very few real world observations follow chi squared. Its main purpose is for hypothesis testing. Its shape depends on value of k.mean of chi-square distribution is k and variance is 2k. range is 0 to infinity.

Relationship of chi square and standard normal distribution. 

Std normal distribution: Normal distribution where mean = 0 and std deviation = 1. Take a random sample of standard distribution, square all the values in the sample, you would have chi squared distribution with k = 1. X1^2 = Z1^2 + Z2^2. Here only 1 std normal distribution. Take samples from two std normal distribution Z1 and Z2, If each time you sampled a pair of values, you squared them and added them together, you would have the chi-square distribution with k = 2. X2^2 = Z1^^2 + Z2^2 

## t-distribution
Also known as student's distribution. it is a type of normal distribution for small sample size and variance of data is unknown. Here the data follows the bell-shaped curve with greatest number of observation close to mean and few or rest observations in the tails of the either side.

When the data are approximately normally distributed but the population variance is unknown. Variance in t-distribution is estimated by total number of observations - 1.

t-score or t-value is number of std deviations away from the mean of the t-distribution.It is the test statistics. It is used to describe that an observation is how far from the mean when data follows t-distribution.


## Inferential statistics

Descriptive allows to summarize the characteristics of a data set. Inferential statistics allows us to come to certain decision or make prediction.
We can use inferential statistics to understand about the larger population from which the sample is taken.
Two use cases:
1. Making estimates about population. (mean weight of 18 y/o in US)
2. Testing hypothesis to draw conclusion about population. (Relationship between weight and fast food consumption)

It is not possible to collect the data of whole population hence we randomly take sample of population we are interested in. Descriptive statistics allows to summarize a sample's characteristics whereas inferential statistics allows to make reasonable guess about the larger population using sample.

The size of sample is always less than that of population size, hence some of population cannot be captured by sample creating sampling error. Sampling error is the difference between true population values (parameters) and measured sample values (statistic).


### How to estimate population parameters from sample statistics

Statistic is the measure that describes the sample and parameter is a measure that describes the whole population. Two types of estimates you can make about the population. 

1. Point estimate: Single value estimate of a parameter. Sample mean is point estimate of a population mean.
2. Interval estimate: Gives you a range of values in which parameter might lie. Confidence interval.

### Confidence interval 
It uses variability (std deviation, range, variance) around a statistic to come up with  a interval estimate for a parameter. They take sampling error into account. Point estimate gives single exact value for a parameter you are intereseted in but interval estimate (confidence interval) tells you the uncertainty of the point estimate. 

Each confidence interval is related with confidence level. 95% CONFIDENCE INTERVAL means when you repeat your experiment with new other sample of same population 100 times, you expect your estimate to lie within specified range of value 95 times. 

For a study about paid vacation days in a company. You took survey from a random sample. You calculate point estimate and confidence interval. Point estimate of all employee in that company states that sample mean of 19 paid vacation employee in the company takes paid leave. 
With random sampling, a 95% confidence interval of [16,22] means you can be confident that average number  of vacation days is between 12 and 16.

### Hypothesis testing
Compare population or assess relationships between variables using sample.  Statistical tests can be either parameteric or non-parametric. Parametric tests can be considered more statistically powerful because they are more likely to detect an effect if one exists. 

Parameteric tests has considerations or assumptions 
1. Population from which the sample is taken follows normal distribution
2. sample size is large enough to represent population
3. variance of each group being compared are similar.

If any of these assumptions is not satisfied then better to do non-parameteric tests. Non-parameteric tests are distribution free test as they do not have assumption anything about distribution of the data.


Parametric or non-parametric, these both tests comes in three forms: 
1. Comparison test: Assess if there are differences in means or medians or rankings of scores of two or more groups. 
2. Correlation test: Assess to which extent two variables are associated. Most powerful test is pearson's r test but if your data do not follow assumption of parametric test then spearman's r test is choosen in case of interval/ratio. 
3. Regression test: Whether the change in predictor variable causes changes in an outcome variable.

Following table presents different comparison test:
 Comparison test | Parametric ? | Comparing statistics | Samples |
| ------------- | ------------- | ------------- | ------------- |
| t test  | Yes | Means | 2 samples ( 2 groups) |
| ANOVA | Yes |  Means  | More than 3 samples |
| Mood's median | No | Medians| 2+ samples |
| Wilcoxon signed-rank | No | Distributions | 2 samples |
| Wilcoxon rank sum  | No  | Sums of rankings | 2 samples | 
| Kruskal Wallis H | No | Mean rankings| 3+ samples |


Following table present different correlation test:

Correlation test | Parametric ? | Variable type |
| ------------- | ------------- | ------------- | 
| Pearson's r  | Yes | Interval or ratio |
| Spearman's r | No | Ordinal/interval/ratio |
| Chi square test of independence  | No | Nominal/ordinal |

## Degrees of freedom
 
Represented by v or df. number of independent pieces of information that is used to calculate statistics. Sample size if small few independent piece of information and only few degrees of freedom. Large sample size, many degrees of freedom.  It is the number of values that are free to vary. When estimating a parameter, such as the mean or variance, there are often constraints or restrictions placed on the data. Degrees of freedom take into account these restrictions. Here free to vary can be explained using example of 5 numbers that sums up to 100. Summing to 100 is the restriction. For 1st number any integer can be choosen, same for 2nd, 3rd and 4th but in 5th number you do not have choice to choose number. The value can only be one possible number that can take place in 5th number. 5th depended upon 1st,2nd, 3rd and 4th number. So, it was not free to vary. 

The reason why we choose n-1 is that we take into account of the fact of bias. That is we are calculating estimate of sample to infer about population. The reason for using n−1 is to correct for the fact that when estimating parameters from a sample, you are using information from the sample itself (e.g., the sample mean). The use of  n−1 instead of n helps to provide an unbiased estimate of the population parameters. It is based on the idea that using n would underestimate the variability in the population. 

When you estimate a parameter, we need to introduce restrictions in how values are related to each other. 
v or df =  sample size - number of restrictions. usually restrictions = number of parameters estimated.

"The participants mean salt intake did not differ the recommended amount of 1000 mg, t(9) = 1.41, p=0.19" This means t = t-test is done where 9 is degrees of freedom and p = 0.19 is p-value that states that the strenght of evidence against null hypothesis. If p-value is less than significance level, we have enough evidence to reject null hypothesis.


Suppose you’re comparing the average monthly rent in New York, Vancouver, and San Francisco using a one-way ANOVA. You randomly sample 20 apartments in each city. How many within-group degrees of freedom does the test statistic have?
Here, k = 3 cities = 3 groups
N = sum of all groups sample size, each group = 20 sample so for 3 groups = 3*20 = 60.
df = N-k = 60-3 = 57

In one way ANOVA df = k-1 for between-groups and df = N-k for within groups. Total df = N-1 for within groups. N = sum of all groups sample size and k  = number of groups. 


## Central limit theorem

If sufficient large samples from population is taken, the samples means will be normally distributed even though population from which samples were taken is not normally distributed.

Draw a random sample from  a population & calculate the statistic for a sample say mean. Now, draw another random sample of same size and again calculate same test statistic i.e mean. You repeat this process many times, You are going to end up with number of means where each mean from each sample. This distribution of sample mean is called sampling distribution. Central limit theorem states that no matter which distribution population is from as long as the sample size is large enough, the sampling distribution of mean will always have normal distribution. Here sufficiently large number of sample means n > or equal to 30.

## Test statistic 

The value calculated from a statistical test of a hypothesis. The summary of sample data used in hypothesis testing. Different statistical test has different formulas to calculate test statistics.  By using certain statistical test, we find a test statistic of sample data. Once this test statistic is calculated, we compare this value with a distribution and this distribution is often called null distribution. (fOR T-TEST, null-distribution is t-distribution, z-test, null -distribution is z-distribution). We are trying to assess whether observed sample data  deviate significantly from what would you expect under null hypothesis is true. Here,we are taking a distribution where null hypothesis is considered to be true (here wer are assuming if that data followed the null hypothesis as true then it would be expected to follow this distribution) and compare it against our test statistic.

It measures the difference between the observed data and  what would be expected under the null hypothesis.

Gist is We will have H0, which states there is no significant difference. We then calculate test statistic. If there was no any difference (H0 is true), then test statistic is expected to follow just by chance(might follow or might not follow, if H0=true and test statistic follows the pattern then there is evidence that H0 is true).  We comparing the test statistic against expected value or pattern. Here expected pattern/value is  based on the distribution followed by the test you have used. If you used t-statistic then we will compare against t-distribution.Then we will check whether calculated statistic follows this expected pattern or not. If the difference of values of t-statistic and expected pattern is too large, it means observed data is less likely to follow that expected distribution/pattern. hence it provides enough evidence to reject H0.When you say "if the difference is very large," you are likely referring to an observed test statistic that is far from what you would expect under the null hypothesis. In the context of hypothesis testing, a large difference or an extreme test statistic suggests that the observed data is unlikely to have occurred by random chance alone.

For example: 
Your calculated t value of 2.36 is far from the expected range of t values under the null hypothesis, and the p value is < 0.01. This means that you would expect to see a t value as large or larger than 2.36 less than 1% of the time if the true relationship between temperature and flowering dates was 0.
Therefore, it is statistically unlikely that your observed data could have occurred under the null hypothesis. Using a significance threshold of 0.05, you can say that the result is statistically significant.

Here, you conducted regression test. The test statistic is t-value = 2.36. Using t-statistic, you calculated p-value (it is the strength of rejecting null hypothesis H0 = THERE IS NO RELATIONSHIP BETWEEN TEMP AND FLOWERING DATES). Using formulas we got p-value less than 0.01 where 0.01 is the significance level. This means that you would expect to see a t value as large or larger than 2.36 less than 1% of the time if the true relationship between temperature and flowering dates was 0. Since we rejected H0, we can say there exists statistically significant relationship between temp and flowering dates.











